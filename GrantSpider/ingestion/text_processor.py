"""
Metin iÅŸleme ve chunking
"""

from typing import List
from langchain_text_splitters import RecursiveCharacterTextSplitter
from langchain_core.documents import Document
from config.settings import settings

class TextProcessor:
    """Metin iÅŸleme ve bÃ¶lme iÅŸlemleri"""
    
    def __init__(self, chunk_size: int = None, chunk_overlap: int = None):
        self.chunk_size = chunk_size or settings.CHUNK_SIZE
        self.chunk_overlap = chunk_overlap or settings.CHUNK_OVERLAP
        
        # RecursiveCharacterTextSplitter oluÅŸtur
        self.text_splitter = RecursiveCharacterTextSplitter(
            chunk_size=self.chunk_size,
            chunk_overlap=self.chunk_overlap,
            length_function=len,
            separators=[
                "\n\n",  # Paragraf ayÄ±rÄ±cÄ±larÄ±
                "\n",    # SatÄ±r ayÄ±rÄ±cÄ±larÄ±
                " ",     # Kelime ayÄ±rÄ±cÄ±larÄ±
                ""       # Karakter ayÄ±rÄ±cÄ±larÄ±
            ],
            keep_separator=True
        )
    
    def process_document(self, document: Document) -> List[Document]:
        """
        Tek bir belgeyi iÅŸler ve parÃ§alara bÃ¶ler
        
        Args:
            document: Ä°ÅŸlenecek LangChain Document
            
        Returns:
            Ä°ÅŸlenmiÅŸ Document parÃ§alarÄ±nÄ±n listesi
        """
        # Metni temizle
        cleaned_text = self._clean_text(document.page_content)
        
        # Metni parÃ§alara bÃ¶l
        chunks = self.text_splitter.split_text(cleaned_text)
        
        # Her parÃ§a iÃ§in yeni Document oluÅŸtur
        processed_documents = []
        for i, chunk in enumerate(chunks):
            # Orijinal metadata'yÄ± kopyala ve chunk bilgilerini ekle
            chunk_metadata = document.metadata.copy()
            chunk_metadata.update({
                "chunk_index": i,
                "total_chunks": len(chunks),
                "chunk_size": len(chunk)
            })
            
            chunk_document = Document(
                page_content=chunk,
                metadata=chunk_metadata
            )
            processed_documents.append(chunk_document)
        
        return processed_documents
    
    def process_documents(self, documents: List[Document]) -> List[Document]:
        """
        Birden fazla belgeyi iÅŸler
        
        Args:
            documents: Ä°ÅŸlenecek Document listesi
            
        Returns:
            TÃ¼m iÅŸlenmiÅŸ Document parÃ§alarÄ±nÄ±n listesi
        """
        print(f"ğŸ“ {len(documents)} belge iÅŸleniyor...")
        
        all_chunks = []
        for i, document in enumerate(documents):
            print(f"ğŸ“„ Ä°ÅŸleniyor: {document.metadata.get('filename', f'Belge {i+1}')}")
            
            chunks = self.process_document(document)
            all_chunks.extend(chunks)
            
            print(f"âœ‚ï¸  {len(chunks)} parÃ§aya bÃ¶lÃ¼ndÃ¼")
        
        print(f"ğŸ‰ Toplam {len(all_chunks)} metin parÃ§asÄ± oluÅŸturuldu")
        return all_chunks
    
    def _clean_text(self, text: str) -> str:
        """
        Metni temizler
        
        Args:
            text: Temizlenecek metin
            
        Returns:
            TemizlenmiÅŸ metin
        """
        if not text:
            return ""
        
        # Fazla boÅŸluklarÄ± ve gereksiz karakterleri temizle
        lines = text.split('\n')
        cleaned_lines = []
        
        for line in lines:
            # SatÄ±r baÅŸÄ± ve sonundaki boÅŸluklarÄ± temizle
            line = line.strip()
            
            # BoÅŸ satÄ±rlarÄ± ve Ã§ok kÄ±sa satÄ±rlarÄ± atla
            if len(line) > 2:
                # Fazla boÅŸluklarÄ± tek boÅŸluÄŸa Ã§evir
                line = ' '.join(line.split())
                cleaned_lines.append(line)
        
        # SatÄ±rlarÄ± birleÅŸtir
        cleaned_text = '\n'.join(cleaned_lines)
        
        # Fazla satÄ±r sonlarÄ±nÄ± temizle
        while '\n\n\n' in cleaned_text:
            cleaned_text = cleaned_text.replace('\n\n\n', '\n\n')
        
        return cleaned_text.strip()
    
    def get_chunk_statistics(self, chunks: List[Document]) -> dict:
        """
        Chunk istatistiklerini dÃ¶ndÃ¼rÃ¼r
        
        Args:
            chunks: Document chunk'larÄ±
            
        Returns:
            Ä°statistik bilgileri
        """
        if not chunks:
            return {"total_chunks": 0}
        
        chunk_sizes = [len(chunk.page_content) for chunk in chunks]
        
        return {
            "total_chunks": len(chunks),
            "average_chunk_size": sum(chunk_sizes) / len(chunks),
            "min_chunk_size": min(chunk_sizes),
            "max_chunk_size": max(chunk_sizes),
            "total_characters": sum(chunk_sizes),
            "documents_processed": len(set(chunk.metadata.get("filename", "") for chunk in chunks))
        } 